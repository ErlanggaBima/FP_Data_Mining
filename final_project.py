# -*- coding: utf-8 -*-
"""Final project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1B7auUQ2131wablYHCRLyLPyyUzhe3MgV
"""

import pandas as pd

url = 'https://drive.google.com/file/d/1Y2YV23TOlgKzBBC6C4Hkkg-MHnzIuU8O/view?usp=sharing' #file awal
path = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]

df = pd.read_csv(path)

import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Load the data
url = 'https://drive.google.com/file/d/1Y2YV23TOlgKzBBC6C4Hkkg-MHnzIuU8O/view?usp=sharing'
path = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]
df = pd.read_csv(path)

# Assuming your data has numerical columns for simplicity
numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns
X = df[numeric_columns]

# Standardize the data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Choose the number of clusters (you may need to tune this parameter)
n_clusters = 3

# Apply K-Means clustering
kmeans = KMeans(n_clusters=n_clusters, random_state=42)
df['cluster'] = kmeans.fit_predict(X_scaled)

# Display the DataFrame with cluster assignments
print(df)

import pandas as pd
from sklearn.preprocessing import LabelEncoder

# Load the DataFrame
url = 'https://drive.google.com/file/d/1Y2YV23TOlgKzBBC6C4Hkkg-MHnzIuU8O/view?usp=sharing'
path = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]
df = pd.read_csv(path)

# Encode 'WHO Region' column
le = LabelEncoder()
le.fit(df['WHO Region'])
df['WHO Region Encoded'] = le.transform(df['WHO Region'])

# Initialize lists
confirmed = []
deaths = []
recovered = []
active = []
new_cases = []
new_deaths = []
who = []

# Loop to fill lists with data from DataFrame
for i in range(len(df)):
    confirmed.append(df.iloc[i]['Confirmed'])
    deaths.append(df.iloc[i]['Deaths'])
    recovered.append(df.iloc[i]['Recovered'])
    active.append(df.iloc[i]['Active'])
    new_cases.append(df.iloc[i]['New cases'])
    new_deaths.append(df.iloc[i]['New deaths'])
    who.append(df.iloc[i]['WHO Region Encoded'])

# Combine lists into one list of tuples (data)
data = list(zip(confirmed, deaths, recovered, active, new_cases, new_deaths, who))
print(data)

import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
import gdown

# Load the DataFrame from Google Drive
url = 'https://drive.google.com/file/d/1Y2YV23TOlgKzBBC6C4Hkkg-MHnzIuU8O/view?usp=sharing'
path = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]
df = pd.read_csv(path)

# Encode 'WHO Region' column
le = LabelEncoder()
df['WHO Region Encoded'] = le.fit_transform(df['WHO Region'])

# Combine numeric columns
data = df[['Confirmed', 'Deaths', 'Recovered', 'Active', 'New cases', 'New deaths', 'WHO Region Encoded']]

# Check the number of samples and clusters
n_samples, n_features = data.shape
max_clusters = 10  # Set the maximum number of clusters you want to visualize

if n_samples < max_clusters:
    raise ValueError("Number of samples must be greater than or equal to the number of clusters.")

# Elbow method to determine the optimal number of clusters
inertias = []
for i in range(1, min(n_samples, max_clusters) + 1):
    kmeans = KMeans(n_clusters=i)
    kmeans.fit(data)
    inertias.append(kmeans.inertia_)

# Visualization of the Elbow method
plt.plot(range(1, min(n_samples, max_clusters) + 1), inertias, marker='o')
plt.title('Elbow method')
plt.xlabel('Number of clusters')
plt.ylabel('Inertia')
plt.show()

# Run KMeans model for cluster numbers 1 to 5
for num_clusters in range(1, 6):
    kmeans = KMeans(n_clusters=num_clusters)
    kmeans.fit(data)

    # Display cluster results
    df[f'Cluster_{num_clusters}'] = kmeans.labels_
    cluster_centers = pd.DataFrame(kmeans.cluster_centers_, columns=data.columns)

    print(f"\nResults for {num_clusters} clusters:")
    print("Cluster Centers:")
    print(cluster_centers)
    print("\nDataFrame with Clusters:")
    print(df[[f'Cluster_{num_clusters}'] + list(data.columns)])

print(df)

#menambahkan kolom label
df.insert(len(df.columns), 'label', kmeans.labels_, True)

print(df)

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import numpy as np
from numpy import array, linspace
from sklearn.neighbors import KernelDensity
from matplotlib.pyplot import plot

Confirmed_to_array = df['Confirmed'].to_numpy()
print(Confirmed_to_array)

import pandas as pd
import numpy as np
from sklearn.cluster import MeanShift, estimate_bandwidth

# Read the CSV file into a DataFrame
url = 'https://drive.google.com/file/d/1Y2YV23TOlgKzBBC6C4Hkkg-MHnzIuU8O/view?usp=sharing'
path = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]
df = pd.read_csv(path)

# Select numerical columns for clustering
numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns
X = df[numeric_columns].to_numpy()

# Compute clustering with MeanShift
bandwidth = estimate_bandwidth(X, quantile=0.2, n_samples=100)

# Check if bandwidth is very close to zero, set it to a small positive value
if bandwidth < 1e-10:
    bandwidth = 1e-10

ms = MeanShift(bandwidth=bandwidth, bin_seeding=True)
ms.fit(X)
labels = ms.labels_
cluster_centers = ms.cluster_centers_

labels_unique = np.unique(labels)
n_clusters_ = len(labels_unique)

# Display the results
print("Number of estimated clusters: %d" % n_clusters_)
print("Labels: ", labels)
print("Cluster Centers: \n", cluster_centers)